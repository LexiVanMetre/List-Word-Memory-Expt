{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd8b62",
   "metadata": {},
   "source": [
    "# Assignment 09: Final Project\n",
    "## Computational Methods in Psychology and Neuroscience\n",
    "### Psychology 4215/7215 --- Fall 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc149951",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "Upon completion of this assignment, students will have:\n",
    "\n",
    "1. Described a list generation process in detail\n",
    "2. Described the experiment details\n",
    "3. Stated two new new hypotheses/questions to test\n",
    "3. Visualized processed data testing these hypotheses\n",
    "4. Performed a statistical analyses to test the hypotheses\n",
    "5. Summarized the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9955e9a",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "Write text (in MarkDown cells) and code (in Code cells) in a Jupyter notebook (after making a copy and renaming it to have your userid in the title --- e.g., A10_Final_Project_mst3k).\n",
    "\n",
    "\n",
    "## Details\n",
    "\n",
    "The goal of the final project is to synthesize material covered in the class and produce part of what would go into an actual scientific publication based on *one or more* of the experiments we ran in the class. Specifically, you will be writing part of the Methods and Results sections.\n",
    "\n",
    "You can copy the basic template code for loading and processing the data from the class lessons. We also outline what each section of your assignment should include. In addition to describing the experiment in detail, you will be developing novel hypotheses we did not test in class and then performing the associated analyses and visualizing the results. As always, make sure to label all figures and be sure to refer to the code in the lesson notebooks as a guide for your analyses.\n",
    "\n",
    "Please feel free to reach out to us on Discord if you have any questions along the way. For example, we're happy to weigh in on whether or not your hypotheses are novel.\n",
    "\n",
    "\n",
    "* ***If you select the project in this notebook, when you are done, save this notebook as HTML (`File -> Download as -> HTML`) and upload it to the matching assignment on UVA Canvas.***  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8d0d6",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7919b3de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ci_within'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbambi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbmb\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmile\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log2dl\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mci_within\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ci_within\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ci_within'"
     ]
    }
   ],
   "source": [
    "# import some useful libraries\n",
    "import random\n",
    "import csv\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as smc\n",
    "from statsmodels.stats.api import anova_lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from smile.common import *\n",
    "from smile.scale import scale as s\n",
    "\n",
    "from smile.common import *\n",
    "from smile.scale import scale as s\n",
    "from smile.math_distract import MathDistract\n",
    "from smile.moving_dots import MovingDots\n",
    "import plotnine as pn\n",
    "import scipy.stats.distributions as dists     # probability distributions\n",
    "from scipy import stats\n",
    "from glob import glob\n",
    "import os\n",
    "import arviz as az\n",
    "import bambi as bmb\n",
    "\n",
    "from smile.log import log2dl\n",
    "\n",
    "from ci_within import ci_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ba662",
   "metadata": {},
   "source": [
    "# Hypotheses\n",
    "\n",
    "1. I hypothesize that words that are presented in the middle of the test are more likely to be forgotten than the first three words presented and the last three words presented. \n",
    "> Serial position effect: Assess whether participants show a tendency to remember items at the beginning (primacy effect) or end (recency effect) of the list more accurately. This can provide insights into how the position of an item in a sequence affects memory recall.\n",
    "2. I hypothesize that if the time difference between the study word presented from the test word is shorter, the participant will have a greater recall and accuracy than if the time series between the study list and test list is greater. \n",
    "> Retention over time: Test participants' ability to retain information over different time intervals. This can include short-term memory (immediate recall) and long-term memory (recall after a delay)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ed4ee",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0291b2e",
   "metadata": {},
   "source": [
    "## List generation\n",
    "\n",
    "*Provide enough detail (in words) about the list generation so that someone could recreate the list generation code themselves. Be sure to state all the specific parameters used, including number of stimuli per block and number of blocks. Refer to the list generation code we provided to find all the information you need.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20398a14",
   "metadata": {},
   "source": [
    "## Refreshing Valence Study\n",
    "\n",
    "The main question of this study is whether recognition memory for\n",
    "words depends on the emotional or affective valence of those words and whether there is an interaction between attention refreshing and valence.\n",
    "\n",
    "Participants will study lists of positive (+), negative (-), and\n",
    "neutral (~) words and then, after a short delay, they will be given a\n",
    "recognition test over all the studied target words plus a matched set\n",
    "of non-studied lures.  The stimuli are contained in three separate CSV\n",
    "files:\n",
    "\n",
    "- [Positive Pool](./pos_pool.csv)\n",
    "- [Negative Pool](./neg_pool.csv)\n",
    "- [Neutral Pool](./neu_pool.csv)\n",
    "\n",
    "You will need to read these files in as lists of dictionaries (hint,\n",
    "use the ``DictReader`` from the ``csv`` module that was covered in\n",
    "class.)  \n",
    "\n",
    "Use these pools to create lists with trials of valence crossed with three experimental conditions:\n",
    "\n",
    "1. *Repeated*: Where a word will be immediately repeated as the next word.\n",
    "2. *Refreshed*: Where you will indicate the participant should \"refresh\" the previous word by presenting a \"+\".\n",
    "3. *Once-presented*: Where a word is only presented once and is *not* repeated or refreshed.\n",
    "\n",
    "We suggest that you generate the study items for a list in two stages. In the first stage you shuffle all combinations of the trial types (Valence crossed with Condition). In the second stage you loop over those conditions and append trials to a block depending on the information in each trial type. For the Repeated and Refreshed you would append two items, for the Once-presented you would only append one.\n",
    "\n",
    "You will need to generate a matching test list for each study list\n",
    "that includes all the studied items, plus a set of lures that match\n",
    "the valence of the studied words.\n",
    "\n",
    "Be sure to add in information to each trial dictionary that identifies\n",
    "the word, its valence, the condition of that trial, and whether it is a\n",
    "target or a lure.  Feel free to add in more information if you would\n",
    "like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ede9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to read in the pools\n",
    "def read_and_shuffle(pool_file):\n",
    "    \"\"\"Read in and shuffle a pool.\"\"\"\n",
    "    # create a dictionary reader\n",
    "    dr = csv.DictReader(open(pool_file, 'r'))\n",
    "\n",
    "    # read in all the lines into a list of dicts\n",
    "    pool = [l for l in dr]\n",
    "\n",
    "    # shuffle it so that the we get new items each time\n",
    "    random.shuffle(pool)\n",
    "    \n",
    "    # report out some pool info\n",
    "    print(pool_file, len(pool))\n",
    "\n",
    "    # return the shuffled pool\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd9304",
   "metadata": {},
   "source": [
    "In creating the list generation, the researcher will first have to code a function. This is done by creating a definition or def name_of_function(x): create a dictionary reader, read in all the lines into a list of dictionaries, shuffle it so that we get new items each time, report out some pool information (by taking the length or len of the pool file, and finally return the shuffled pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23dce0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Section\n",
    "pool_files = {'pos': 'pos_pool.csv',\n",
    "              'neg': 'neg_pool.csv',\n",
    "              'neu': 'neu_pool.csv'}\n",
    "\n",
    "rep_conds = ['once', 'repeat', 'refresh']\n",
    "val_conds = ['pos', 'neg', 'neu']\n",
    "\n",
    "# what to show for refreshed items\n",
    "ref_text = '+'\n",
    "\n",
    "num_reps = 1\n",
    "num_blocks = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef2d66",
   "metadata": {},
   "source": [
    "In the next section of code, the researcher will define all of the terms for the rest of the experiment. You will first create a dictionary that allows for pos to represent the csv file that contains the positive word pool. Next, the researcher will define the valence conditions. For example, val_cond is given the attributed pos, neg, or neu. They also define what the repition conditions are and what should appear when a word is refreshed (+). The differing repition condtions should be once, repeated, or refreshed. It also is defining the number of reps as one and the number of blocks as one. These numbers for the number of reps and the number of blocks can easily be changed for anyone wanting to reproduce the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9159f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_pool.csv 301\n",
      "neg_pool.csv 292\n",
      "neu_pool.csv 208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'description': 'leader',\n",
       "  'word_no': '844',\n",
       "  'valence_mean': '7.6299999999999999',\n",
       "  'valence_sd': '1.5900000000000001',\n",
       "  'arousal_mean': '6.2699999999999996',\n",
       "  'arousal_sd': '2.1800000000000002',\n",
       "  'dominance_mean': '7.8799999999999999',\n",
       "  'dominance_sd': '1.6000000000000001',\n",
       "  'word_frequency': '74'},\n",
       " {'description': 'restaurant',\n",
       "  'word_no': '960',\n",
       "  'valence_mean': '6.7599999999999998',\n",
       "  'valence_sd': '1.8500000000000001',\n",
       "  'arousal_mean': '5.4100000000000001',\n",
       "  'arousal_sd': '2.5499999999999998',\n",
       "  'dominance_mean': '5.7300000000000004',\n",
       "  'dominance_sd': '1.4099999999999999',\n",
       "  'word_frequency': '41'},\n",
       " {'description': 'vision',\n",
       "  'word_no': '480',\n",
       "  'valence_mean': '6.6200000000000001',\n",
       "  'valence_sd': '1.8400000000000001',\n",
       "  'arousal_mean': '4.6600000000000001',\n",
       "  'arousal_sd': '2.4300000000000002',\n",
       "  'dominance_mean': '6.0199999999999996',\n",
       "  'dominance_sd': '1.96',\n",
       "  'word_frequency': '56'},\n",
       " {'description': 'silly',\n",
       "  'word_no': '981',\n",
       "  'valence_mean': '7.4100000000000001',\n",
       "  'valence_sd': '1.8',\n",
       "  'arousal_mean': '5.8799999999999999',\n",
       "  'arousal_sd': '2.3799999999999999',\n",
       "  'dominance_mean': '6.0',\n",
       "  'dominance_sd': '2.0899999999999999',\n",
       "  'word_frequency': '15'},\n",
       " {'description': 'twilight',\n",
       "  'word_no': '1022',\n",
       "  'valence_mean': '7.2300000000000004',\n",
       "  'valence_sd': '1.8',\n",
       "  'arousal_mean': '4.7000000000000002',\n",
       "  'arousal_sd': '2.4100000000000001',\n",
       "  'dominance_mean': '5.5899999999999999',\n",
       "  'dominance_sd': '1.8200000000000001',\n",
       "  'word_frequency': '4'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the pools into a dictionary\n",
    "pools = {val: read_and_shuffle(pool_files[val])\n",
    "         for val in val_conds}\n",
    "\n",
    "# show the first 5 items of the pos pool\n",
    "pools['pos'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15b6d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'valence': 'pos', 'cond': 'once'},\n",
       " {'valence': 'pos', 'cond': 'repeat'},\n",
       " {'valence': 'pos', 'cond': 'refresh'},\n",
       " {'valence': 'neg', 'cond': 'once'},\n",
       " {'valence': 'neg', 'cond': 'repeat'},\n",
       " {'valence': 'neg', 'cond': 'refresh'},\n",
       " {'valence': 'neu', 'cond': 'once'},\n",
       " {'valence': 'neu', 'cond': 'repeat'},\n",
       " {'valence': 'neu', 'cond': 'refresh'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the conds\n",
    "# fully crossed with all combos of val and rep\n",
    "conds = []\n",
    "for val in val_conds:\n",
    "    for rep in rep_conds:\n",
    "        # I decided to call the repetition condition cond\n",
    "        conds.append({'valence': val, 'cond': rep})\n",
    "conds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f922ba",
   "metadata": {},
   "source": [
    "In this section of code, the researcher is producing all the viable configurations of a word, it's repitition, and it's valence. For example, it could be a positive word, repeated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4068d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function for generating a block\n",
    "# with a study and test list\n",
    "def make_block():\n",
    "    \"\"\"Generate a block, uses global variables\"\"\"\n",
    "    # loop and create the repeated conditions\n",
    "    block_conds = []\n",
    "    for i in range(num_reps):\n",
    "        # extend the trials with copies of the conditions\n",
    "        block_conds.extend(deepcopy(conds))\n",
    "\n",
    "    # shuffle the conds for that block\n",
    "    random.shuffle(block_conds)\n",
    "\n",
    "    # loop over block conds and add items to study/test lists\n",
    "    study_list = []\n",
    "    test_list = []\n",
    "    for cond in block_conds:\n",
    "        # use the valence to grab study and test items\n",
    "        study_item = pools[cond['valence']].pop()\n",
    "        test_item = pools[cond['valence']].pop()\n",
    "\n",
    "        # update with the cond info\n",
    "        study_item.update(cond)\n",
    "        test_item.update(cond)\n",
    "\n",
    "        # add in relevant info for study and test\n",
    "        study_item['pres_num'] = 1\n",
    "        study_item['type'] = 'target'\n",
    "        test_item['type'] = 'lure'\n",
    "        test_item['pres_num'] = 1   # just so the keys match\n",
    "\n",
    "        # append them to the respective lists\n",
    "        # study item is added to both study and test\n",
    "        study_list.append(study_item)\n",
    "        test_list.append(study_item)\n",
    "        test_list.append(test_item)\n",
    "\n",
    "        # process the repetition and refresh conditions\n",
    "        if cond['cond'] in ['refresh', 'repeat']:\n",
    "            # copy the study item\n",
    "            rep_item = deepcopy(study_item)\n",
    "\n",
    "            # modify required values\n",
    "            rep_item['pres_num'] = 2\n",
    "\n",
    "            # change the description if refreshing\n",
    "            if cond['cond'] == 'refresh':\n",
    "                rep_item['description'] = ref_text\n",
    "\n",
    "            # append it to the study list\n",
    "            study_list.append(rep_item)\n",
    "            \n",
    "    # must shuffle the test list\n",
    "    random.shuffle(test_list)\n",
    "    \n",
    "    # make a dictionary to return\n",
    "    block = {'study': study_list, 'test': test_list}\n",
    "    \n",
    "    return block\n",
    "\n",
    "# generate the proper number of blocks\n",
    "blocks = []\n",
    "for b in range(num_blocks):\n",
    "    blocks.append(make_block())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0cdae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'study': [{'description': 'avenue',\n",
       "    'word_no': '646',\n",
       "    'valence_mean': '5.5',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.0099999999999998',\n",
       "    'dominance_mean': '5.4000000000000004',\n",
       "    'dominance_sd': '1.53',\n",
       "    'word_frequency': '46',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'basket',\n",
       "    'word_no': '547',\n",
       "    'valence_mean': '5.4500000000000002',\n",
       "    'valence_sd': '1.1499999999999999',\n",
       "    'arousal_mean': '3.6299999999999999',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.7599999999999998',\n",
       "    'dominance_sd': '1.45',\n",
       "    'word_frequency': '17',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': '+',\n",
       "    'word_no': '547',\n",
       "    'valence_mean': '5.4500000000000002',\n",
       "    'valence_sd': '1.1499999999999999',\n",
       "    'arousal_mean': '3.6299999999999999',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.7599999999999998',\n",
       "    'dominance_sd': '1.45',\n",
       "    'word_frequency': '17',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'habit',\n",
       "    'word_no': '775',\n",
       "    'valence_mean': '4.1100000000000003',\n",
       "    'valence_sd': '1.77',\n",
       "    'arousal_mean': '3.9500000000000002',\n",
       "    'arousal_sd': '2.1099999999999999',\n",
       "    'dominance_mean': '4.2999999999999998',\n",
       "    'dominance_sd': '1.79',\n",
       "    'word_frequency': '23',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'habit',\n",
       "    'word_no': '775',\n",
       "    'valence_mean': '4.1100000000000003',\n",
       "    'valence_sd': '1.77',\n",
       "    'arousal_mean': '3.9500000000000002',\n",
       "    'arousal_sd': '2.1099999999999999',\n",
       "    'dominance_mean': '4.2999999999999998',\n",
       "    'dominance_sd': '1.79',\n",
       "    'word_frequency': '23',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dump',\n",
       "    'word_no': '733',\n",
       "    'valence_mean': '3.21',\n",
       "    'valence_sd': '1.8700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.3599999999999999',\n",
       "    'dominance_mean': '3.8300000000000001',\n",
       "    'dominance_sd': '1.8700000000000001',\n",
       "    'word_frequency': '4',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'lust',\n",
       "    'word_no': '519',\n",
       "    'valence_mean': '7.1200000000000001',\n",
       "    'valence_sd': '1.6200000000000001',\n",
       "    'arousal_mean': '6.8799999999999999',\n",
       "    'arousal_sd': '1.8500000000000001',\n",
       "    'dominance_mean': '5.4900000000000002',\n",
       "    'dominance_sd': '2.27',\n",
       "    'word_frequency': '5',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'lust',\n",
       "    'word_no': '519',\n",
       "    'valence_mean': '7.1200000000000001',\n",
       "    'valence_sd': '1.6200000000000001',\n",
       "    'arousal_mean': '6.8799999999999999',\n",
       "    'arousal_sd': '1.8500000000000001',\n",
       "    'dominance_mean': '5.4900000000000002',\n",
       "    'dominance_sd': '2.27',\n",
       "    'word_frequency': '5',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'alimony',\n",
       "    'word_no': '634',\n",
       "    'valence_mean': '3.9500000000000002',\n",
       "    'valence_sd': '2.0',\n",
       "    'arousal_mean': '4.2999999999999998',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '4.6299999999999999',\n",
       "    'dominance_sd': '2.2999999999999998',\n",
       "    'word_frequency': '2',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': '+',\n",
       "    'word_no': '634',\n",
       "    'valence_mean': '3.9500000000000002',\n",
       "    'valence_sd': '2.0',\n",
       "    'arousal_mean': '4.2999999999999998',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '4.6299999999999999',\n",
       "    'dominance_sd': '2.2999999999999998',\n",
       "    'word_frequency': '2',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dignified',\n",
       "    'word_no': '118',\n",
       "    'valence_mean': '7.0999999999999996',\n",
       "    'valence_sd': '1.26',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '6.1200000000000001',\n",
       "    'dominance_sd': '2.3999999999999999',\n",
       "    'word_frequency': '7',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'city',\n",
       "    'word_no': '73',\n",
       "    'valence_mean': '6.0300000000000002',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '5.2400000000000002',\n",
       "    'arousal_sd': '2.5299999999999998',\n",
       "    'dominance_mean': '5.7400000000000002',\n",
       "    'dominance_sd': '2.0800000000000001',\n",
       "    'word_frequency': '393',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'city',\n",
       "    'word_no': '73',\n",
       "    'valence_mean': '6.0300000000000002',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '5.2400000000000002',\n",
       "    'arousal_sd': '2.5299999999999998',\n",
       "    'dominance_mean': '5.7400000000000002',\n",
       "    'dominance_sd': '2.0800000000000001',\n",
       "    'word_frequency': '393',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'powerful',\n",
       "    'word_no': '324',\n",
       "    'valence_mean': '6.8399999999999999',\n",
       "    'valence_sd': '1.8',\n",
       "    'arousal_mean': '5.8300000000000001',\n",
       "    'arousal_sd': '2.6899999999999999',\n",
       "    'dominance_mean': '7.1900000000000004',\n",
       "    'dominance_sd': '2.52',\n",
       "    'word_frequency': '63',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': '+',\n",
       "    'word_no': '324',\n",
       "    'valence_mean': '6.8399999999999999',\n",
       "    'valence_sd': '1.8',\n",
       "    'arousal_mean': '5.8300000000000001',\n",
       "    'arousal_sd': '2.6899999999999999',\n",
       "    'dominance_mean': '7.1900000000000004',\n",
       "    'dominance_sd': '2.52',\n",
       "    'word_frequency': '63',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'}],\n",
       "  'test': [{'description': 'city',\n",
       "    'word_no': '73',\n",
       "    'valence_mean': '6.0300000000000002',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '5.2400000000000002',\n",
       "    'arousal_sd': '2.5299999999999998',\n",
       "    'dominance_mean': '5.7400000000000002',\n",
       "    'dominance_sd': '2.0800000000000001',\n",
       "    'word_frequency': '393',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'habit',\n",
       "    'word_no': '775',\n",
       "    'valence_mean': '4.1100000000000003',\n",
       "    'valence_sd': '1.77',\n",
       "    'arousal_mean': '3.9500000000000002',\n",
       "    'arousal_sd': '2.1099999999999999',\n",
       "    'dominance_mean': '4.2999999999999998',\n",
       "    'dominance_sd': '1.79',\n",
       "    'word_frequency': '23',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dazzle',\n",
       "    'word_no': '717',\n",
       "    'valence_mean': '7.29',\n",
       "    'valence_sd': '1.0900000000000001',\n",
       "    'arousal_mean': '6.3300000000000001',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.6200000000000001',\n",
       "    'dominance_sd': '1.8100000000000001',\n",
       "    'word_frequency': '1',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'crisis',\n",
       "    'word_no': '706',\n",
       "    'valence_mean': '2.7400000000000002',\n",
       "    'valence_sd': '2.23',\n",
       "    'arousal_mean': '5.4400000000000004',\n",
       "    'arousal_sd': '3.0699999999999998',\n",
       "    'dominance_mean': '3.6000000000000001',\n",
       "    'dominance_sd': '2.4700000000000002',\n",
       "    'word_frequency': '82',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'army',\n",
       "    'word_no': '23',\n",
       "    'valence_mean': '4.7199999999999998',\n",
       "    'valence_sd': '1.75',\n",
       "    'arousal_mean': '5.0300000000000002',\n",
       "    'arousal_sd': '2.0299999999999998',\n",
       "    'dominance_mean': '5.0300000000000002',\n",
       "    'dominance_sd': '2.4500000000000002',\n",
       "    'word_frequency': '132',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'dignified',\n",
       "    'word_no': '118',\n",
       "    'valence_mean': '7.0999999999999996',\n",
       "    'valence_sd': '1.26',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '6.1200000000000001',\n",
       "    'dominance_sd': '2.3999999999999999',\n",
       "    'word_frequency': '7',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'muscular',\n",
       "    'word_no': '290',\n",
       "    'valence_mean': '6.8200000000000003',\n",
       "    'valence_sd': '1.6299999999999999',\n",
       "    'arousal_mean': '5.4699999999999998',\n",
       "    'arousal_sd': '2.2000000000000002',\n",
       "    'dominance_mean': '6.5800000000000001',\n",
       "    'dominance_sd': '2.2799999999999998',\n",
       "    'word_frequency': '16',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'family',\n",
       "    'word_no': '158',\n",
       "    'valence_mean': '7.6500000000000004',\n",
       "    'valence_sd': '1.55',\n",
       "    'arousal_mean': '4.7999999999999998',\n",
       "    'arousal_sd': '2.71',\n",
       "    'dominance_mean': '6.0',\n",
       "    'dominance_sd': '1.8700000000000001',\n",
       "    'word_frequency': '331',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'once',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'rigid',\n",
       "    'word_no': '963',\n",
       "    'valence_mean': '3.6600000000000001',\n",
       "    'valence_sd': '2.1200000000000001',\n",
       "    'arousal_mean': '4.6600000000000001',\n",
       "    'arousal_sd': '2.4700000000000002',\n",
       "    'dominance_mean': '4.6100000000000003',\n",
       "    'dominance_sd': '2.04',\n",
       "    'word_frequency': '24',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'powerful',\n",
       "    'word_no': '324',\n",
       "    'valence_mean': '6.8399999999999999',\n",
       "    'valence_sd': '1.8',\n",
       "    'arousal_mean': '5.8300000000000001',\n",
       "    'arousal_sd': '2.6899999999999999',\n",
       "    'dominance_mean': '7.1900000000000004',\n",
       "    'dominance_sd': '2.52',\n",
       "    'word_frequency': '63',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'python',\n",
       "    'word_no': '949',\n",
       "    'valence_mean': '4.0499999999999998',\n",
       "    'valence_sd': '2.48',\n",
       "    'arousal_mean': '6.1799999999999997',\n",
       "    'arousal_sd': '2.25',\n",
       "    'dominance_mean': '4.5199999999999996',\n",
       "    'dominance_sd': '2.5600000000000001',\n",
       "    'word_frequency': '14',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'once',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'basket',\n",
       "    'word_no': '547',\n",
       "    'valence_mean': '5.4500000000000002',\n",
       "    'valence_sd': '1.1499999999999999',\n",
       "    'arousal_mean': '3.6299999999999999',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.7599999999999998',\n",
       "    'dominance_sd': '1.45',\n",
       "    'word_frequency': '17',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'alimony',\n",
       "    'word_no': '634',\n",
       "    'valence_mean': '3.9500000000000002',\n",
       "    'valence_sd': '2.0',\n",
       "    'arousal_mean': '4.2999999999999998',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '4.6299999999999999',\n",
       "    'dominance_sd': '2.2999999999999998',\n",
       "    'word_frequency': '2',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'lust',\n",
       "    'word_no': '519',\n",
       "    'valence_mean': '7.1200000000000001',\n",
       "    'valence_sd': '1.6200000000000001',\n",
       "    'arousal_mean': '6.8799999999999999',\n",
       "    'arousal_sd': '1.8500000000000001',\n",
       "    'dominance_mean': '5.4900000000000002',\n",
       "    'dominance_sd': '2.27',\n",
       "    'word_frequency': '5',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dump',\n",
       "    'word_no': '733',\n",
       "    'valence_mean': '3.21',\n",
       "    'valence_sd': '1.8700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.3599999999999999',\n",
       "    'dominance_mean': '3.8300000000000001',\n",
       "    'dominance_sd': '1.8700000000000001',\n",
       "    'word_frequency': '4',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'passage',\n",
       "    'word_no': '928',\n",
       "    'valence_mean': '5.2800000000000002',\n",
       "    'valence_sd': '1.4399999999999999',\n",
       "    'arousal_mean': '4.3600000000000003',\n",
       "    'arousal_sd': '2.1299999999999999',\n",
       "    'dominance_mean': '5.0199999999999996',\n",
       "    'dominance_sd': '1.6200000000000001',\n",
       "    'word_frequency': '49',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'salute',\n",
       "    'word_no': '370',\n",
       "    'valence_mean': '5.9199999999999999',\n",
       "    'valence_sd': '1.5700000000000001',\n",
       "    'arousal_mean': '5.3099999999999996',\n",
       "    'arousal_sd': '2.23',\n",
       "    'dominance_mean': '5.46',\n",
       "    'dominance_sd': '2.0499999999999998',\n",
       "    'word_frequency': '3',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'once',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'avenue',\n",
       "    'word_no': '646',\n",
       "    'valence_mean': '5.5',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.0099999999999998',\n",
       "    'dominance_mean': '5.4000000000000004',\n",
       "    'dominance_sd': '1.53',\n",
       "    'word_frequency': '46',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'}]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b7a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the blocks out to a pickle file\n",
    "# (note the 'b' in the 'wb', which means a \n",
    "# binary stream instead of a ascii text stream)\n",
    "pickle.dump(blocks, open('refresh_blocks.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a84f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'study': [{'description': 'avenue',\n",
       "    'word_no': '646',\n",
       "    'valence_mean': '5.5',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.0099999999999998',\n",
       "    'dominance_mean': '5.4000000000000004',\n",
       "    'dominance_sd': '1.53',\n",
       "    'word_frequency': '46',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'basket',\n",
       "    'word_no': '547',\n",
       "    'valence_mean': '5.4500000000000002',\n",
       "    'valence_sd': '1.1499999999999999',\n",
       "    'arousal_mean': '3.6299999999999999',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.7599999999999998',\n",
       "    'dominance_sd': '1.45',\n",
       "    'word_frequency': '17',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': '+',\n",
       "    'word_no': '547',\n",
       "    'valence_mean': '5.4500000000000002',\n",
       "    'valence_sd': '1.1499999999999999',\n",
       "    'arousal_mean': '3.6299999999999999',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.7599999999999998',\n",
       "    'dominance_sd': '1.45',\n",
       "    'word_frequency': '17',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'habit',\n",
       "    'word_no': '775',\n",
       "    'valence_mean': '4.1100000000000003',\n",
       "    'valence_sd': '1.77',\n",
       "    'arousal_mean': '3.9500000000000002',\n",
       "    'arousal_sd': '2.1099999999999999',\n",
       "    'dominance_mean': '4.2999999999999998',\n",
       "    'dominance_sd': '1.79',\n",
       "    'word_frequency': '23',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'habit',\n",
       "    'word_no': '775',\n",
       "    'valence_mean': '4.1100000000000003',\n",
       "    'valence_sd': '1.77',\n",
       "    'arousal_mean': '3.9500000000000002',\n",
       "    'arousal_sd': '2.1099999999999999',\n",
       "    'dominance_mean': '4.2999999999999998',\n",
       "    'dominance_sd': '1.79',\n",
       "    'word_frequency': '23',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dump',\n",
       "    'word_no': '733',\n",
       "    'valence_mean': '3.21',\n",
       "    'valence_sd': '1.8700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.3599999999999999',\n",
       "    'dominance_mean': '3.8300000000000001',\n",
       "    'dominance_sd': '1.8700000000000001',\n",
       "    'word_frequency': '4',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'lust',\n",
       "    'word_no': '519',\n",
       "    'valence_mean': '7.1200000000000001',\n",
       "    'valence_sd': '1.6200000000000001',\n",
       "    'arousal_mean': '6.8799999999999999',\n",
       "    'arousal_sd': '1.8500000000000001',\n",
       "    'dominance_mean': '5.4900000000000002',\n",
       "    'dominance_sd': '2.27',\n",
       "    'word_frequency': '5',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'lust',\n",
       "    'word_no': '519',\n",
       "    'valence_mean': '7.1200000000000001',\n",
       "    'valence_sd': '1.6200000000000001',\n",
       "    'arousal_mean': '6.8799999999999999',\n",
       "    'arousal_sd': '1.8500000000000001',\n",
       "    'dominance_mean': '5.4900000000000002',\n",
       "    'dominance_sd': '2.27',\n",
       "    'word_frequency': '5',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'alimony',\n",
       "    'word_no': '634',\n",
       "    'valence_mean': '3.9500000000000002',\n",
       "    'valence_sd': '2.0',\n",
       "    'arousal_mean': '4.2999999999999998',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '4.6299999999999999',\n",
       "    'dominance_sd': '2.2999999999999998',\n",
       "    'word_frequency': '2',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': '+',\n",
       "    'word_no': '634',\n",
       "    'valence_mean': '3.9500000000000002',\n",
       "    'valence_sd': '2.0',\n",
       "    'arousal_mean': '4.2999999999999998',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '4.6299999999999999',\n",
       "    'dominance_sd': '2.2999999999999998',\n",
       "    'word_frequency': '2',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dignified',\n",
       "    'word_no': '118',\n",
       "    'valence_mean': '7.0999999999999996',\n",
       "    'valence_sd': '1.26',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '6.1200000000000001',\n",
       "    'dominance_sd': '2.3999999999999999',\n",
       "    'word_frequency': '7',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'city',\n",
       "    'word_no': '73',\n",
       "    'valence_mean': '6.0300000000000002',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '5.2400000000000002',\n",
       "    'arousal_sd': '2.5299999999999998',\n",
       "    'dominance_mean': '5.7400000000000002',\n",
       "    'dominance_sd': '2.0800000000000001',\n",
       "    'word_frequency': '393',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'city',\n",
       "    'word_no': '73',\n",
       "    'valence_mean': '6.0300000000000002',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '5.2400000000000002',\n",
       "    'arousal_sd': '2.5299999999999998',\n",
       "    'dominance_mean': '5.7400000000000002',\n",
       "    'dominance_sd': '2.0800000000000001',\n",
       "    'word_frequency': '393',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'},\n",
       "   {'description': 'powerful',\n",
       "    'word_no': '324',\n",
       "    'valence_mean': '6.8399999999999999',\n",
       "    'valence_sd': '1.8',\n",
       "    'arousal_mean': '5.8300000000000001',\n",
       "    'arousal_sd': '2.6899999999999999',\n",
       "    'dominance_mean': '7.1900000000000004',\n",
       "    'dominance_sd': '2.52',\n",
       "    'word_frequency': '63',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': '+',\n",
       "    'word_no': '324',\n",
       "    'valence_mean': '6.8399999999999999',\n",
       "    'valence_sd': '1.8',\n",
       "    'arousal_mean': '5.8300000000000001',\n",
       "    'arousal_sd': '2.6899999999999999',\n",
       "    'dominance_mean': '7.1900000000000004',\n",
       "    'dominance_sd': '2.52',\n",
       "    'word_frequency': '63',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 2,\n",
       "    'type': 'target'}],\n",
       "  'test': [{'description': 'city',\n",
       "    'word_no': '73',\n",
       "    'valence_mean': '6.0300000000000002',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '5.2400000000000002',\n",
       "    'arousal_sd': '2.5299999999999998',\n",
       "    'dominance_mean': '5.7400000000000002',\n",
       "    'dominance_sd': '2.0800000000000001',\n",
       "    'word_frequency': '393',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'habit',\n",
       "    'word_no': '775',\n",
       "    'valence_mean': '4.1100000000000003',\n",
       "    'valence_sd': '1.77',\n",
       "    'arousal_mean': '3.9500000000000002',\n",
       "    'arousal_sd': '2.1099999999999999',\n",
       "    'dominance_mean': '4.2999999999999998',\n",
       "    'dominance_sd': '1.79',\n",
       "    'word_frequency': '23',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dazzle',\n",
       "    'word_no': '717',\n",
       "    'valence_mean': '7.29',\n",
       "    'valence_sd': '1.0900000000000001',\n",
       "    'arousal_mean': '6.3300000000000001',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.6200000000000001',\n",
       "    'dominance_sd': '1.8100000000000001',\n",
       "    'word_frequency': '1',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'crisis',\n",
       "    'word_no': '706',\n",
       "    'valence_mean': '2.7400000000000002',\n",
       "    'valence_sd': '2.23',\n",
       "    'arousal_mean': '5.4400000000000004',\n",
       "    'arousal_sd': '3.0699999999999998',\n",
       "    'dominance_mean': '3.6000000000000001',\n",
       "    'dominance_sd': '2.4700000000000002',\n",
       "    'word_frequency': '82',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'repeat',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'army',\n",
       "    'word_no': '23',\n",
       "    'valence_mean': '4.7199999999999998',\n",
       "    'valence_sd': '1.75',\n",
       "    'arousal_mean': '5.0300000000000002',\n",
       "    'arousal_sd': '2.0299999999999998',\n",
       "    'dominance_mean': '5.0300000000000002',\n",
       "    'dominance_sd': '2.4500000000000002',\n",
       "    'word_frequency': '132',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'repeat',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'dignified',\n",
       "    'word_no': '118',\n",
       "    'valence_mean': '7.0999999999999996',\n",
       "    'valence_sd': '1.26',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '6.1200000000000001',\n",
       "    'dominance_sd': '2.3999999999999999',\n",
       "    'word_frequency': '7',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'muscular',\n",
       "    'word_no': '290',\n",
       "    'valence_mean': '6.8200000000000003',\n",
       "    'valence_sd': '1.6299999999999999',\n",
       "    'arousal_mean': '5.4699999999999998',\n",
       "    'arousal_sd': '2.2000000000000002',\n",
       "    'dominance_mean': '6.5800000000000001',\n",
       "    'dominance_sd': '2.2799999999999998',\n",
       "    'word_frequency': '16',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'family',\n",
       "    'word_no': '158',\n",
       "    'valence_mean': '7.6500000000000004',\n",
       "    'valence_sd': '1.55',\n",
       "    'arousal_mean': '4.7999999999999998',\n",
       "    'arousal_sd': '2.71',\n",
       "    'dominance_mean': '6.0',\n",
       "    'dominance_sd': '1.8700000000000001',\n",
       "    'word_frequency': '331',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'once',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'rigid',\n",
       "    'word_no': '963',\n",
       "    'valence_mean': '3.6600000000000001',\n",
       "    'valence_sd': '2.1200000000000001',\n",
       "    'arousal_mean': '4.6600000000000001',\n",
       "    'arousal_sd': '2.4700000000000002',\n",
       "    'dominance_mean': '4.6100000000000003',\n",
       "    'dominance_sd': '2.04',\n",
       "    'word_frequency': '24',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'powerful',\n",
       "    'word_no': '324',\n",
       "    'valence_mean': '6.8399999999999999',\n",
       "    'valence_sd': '1.8',\n",
       "    'arousal_mean': '5.8300000000000001',\n",
       "    'arousal_sd': '2.6899999999999999',\n",
       "    'dominance_mean': '7.1900000000000004',\n",
       "    'dominance_sd': '2.52',\n",
       "    'word_frequency': '63',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'python',\n",
       "    'word_no': '949',\n",
       "    'valence_mean': '4.0499999999999998',\n",
       "    'valence_sd': '2.48',\n",
       "    'arousal_mean': '6.1799999999999997',\n",
       "    'arousal_sd': '2.25',\n",
       "    'dominance_mean': '4.5199999999999996',\n",
       "    'dominance_sd': '2.5600000000000001',\n",
       "    'word_frequency': '14',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'once',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'basket',\n",
       "    'word_no': '547',\n",
       "    'valence_mean': '5.4500000000000002',\n",
       "    'valence_sd': '1.1499999999999999',\n",
       "    'arousal_mean': '3.6299999999999999',\n",
       "    'arousal_sd': '2.02',\n",
       "    'dominance_mean': '5.7599999999999998',\n",
       "    'dominance_sd': '1.45',\n",
       "    'word_frequency': '17',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'alimony',\n",
       "    'word_no': '634',\n",
       "    'valence_mean': '3.9500000000000002',\n",
       "    'valence_sd': '2.0',\n",
       "    'arousal_mean': '4.2999999999999998',\n",
       "    'arousal_sd': '2.29',\n",
       "    'dominance_mean': '4.6299999999999999',\n",
       "    'dominance_sd': '2.2999999999999998',\n",
       "    'word_frequency': '2',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'refresh',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'lust',\n",
       "    'word_no': '519',\n",
       "    'valence_mean': '7.1200000000000001',\n",
       "    'valence_sd': '1.6200000000000001',\n",
       "    'arousal_mean': '6.8799999999999999',\n",
       "    'arousal_sd': '1.8500000000000001',\n",
       "    'dominance_mean': '5.4900000000000002',\n",
       "    'dominance_sd': '2.27',\n",
       "    'word_frequency': '5',\n",
       "    'valence': 'pos',\n",
       "    'cond': 'repeat',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'dump',\n",
       "    'word_no': '733',\n",
       "    'valence_mean': '3.21',\n",
       "    'valence_sd': '1.8700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.3599999999999999',\n",
       "    'dominance_mean': '3.8300000000000001',\n",
       "    'dominance_sd': '1.8700000000000001',\n",
       "    'word_frequency': '4',\n",
       "    'valence': 'neg',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'},\n",
       "   {'description': 'passage',\n",
       "    'word_no': '928',\n",
       "    'valence_mean': '5.2800000000000002',\n",
       "    'valence_sd': '1.4399999999999999',\n",
       "    'arousal_mean': '4.3600000000000003',\n",
       "    'arousal_sd': '2.1299999999999999',\n",
       "    'dominance_mean': '5.0199999999999996',\n",
       "    'dominance_sd': '1.6200000000000001',\n",
       "    'word_frequency': '49',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'refresh',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'salute',\n",
       "    'word_no': '370',\n",
       "    'valence_mean': '5.9199999999999999',\n",
       "    'valence_sd': '1.5700000000000001',\n",
       "    'arousal_mean': '5.3099999999999996',\n",
       "    'arousal_sd': '2.23',\n",
       "    'dominance_mean': '5.46',\n",
       "    'dominance_sd': '2.0499999999999998',\n",
       "    'word_frequency': '3',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'once',\n",
       "    'type': 'lure',\n",
       "    'pres_num': 1},\n",
       "   {'description': 'avenue',\n",
       "    'word_no': '646',\n",
       "    'valence_mean': '5.5',\n",
       "    'valence_sd': '1.3700000000000001',\n",
       "    'arousal_mean': '4.1200000000000001',\n",
       "    'arousal_sd': '2.0099999999999998',\n",
       "    'dominance_mean': '5.4000000000000004',\n",
       "    'dominance_sd': '1.53',\n",
       "    'word_frequency': '46',\n",
       "    'valence': 'neu',\n",
       "    'cond': 'once',\n",
       "    'pres_num': 1,\n",
       "    'type': 'target'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how to read it back in\n",
    "my_blocks = pickle.load(open('refresh_blocks.pickle','rb'))\n",
    "my_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f50181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>word_no</th>\n",
       "      <th>valence_mean</th>\n",
       "      <th>valence_sd</th>\n",
       "      <th>arousal_mean</th>\n",
       "      <th>arousal_sd</th>\n",
       "      <th>dominance_mean</th>\n",
       "      <th>dominance_sd</th>\n",
       "      <th>word_frequency</th>\n",
       "      <th>valence</th>\n",
       "      <th>cond</th>\n",
       "      <th>pres_num</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avenue</td>\n",
       "      <td>646</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.3700000000000001</td>\n",
       "      <td>4.1200000000000001</td>\n",
       "      <td>2.0099999999999998</td>\n",
       "      <td>5.4000000000000004</td>\n",
       "      <td>1.53</td>\n",
       "      <td>46</td>\n",
       "      <td>neu</td>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basket</td>\n",
       "      <td>547</td>\n",
       "      <td>5.4500000000000002</td>\n",
       "      <td>1.1499999999999999</td>\n",
       "      <td>3.6299999999999999</td>\n",
       "      <td>2.02</td>\n",
       "      <td>5.7599999999999998</td>\n",
       "      <td>1.45</td>\n",
       "      <td>17</td>\n",
       "      <td>neu</td>\n",
       "      <td>refresh</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+</td>\n",
       "      <td>547</td>\n",
       "      <td>5.4500000000000002</td>\n",
       "      <td>1.1499999999999999</td>\n",
       "      <td>3.6299999999999999</td>\n",
       "      <td>2.02</td>\n",
       "      <td>5.7599999999999998</td>\n",
       "      <td>1.45</td>\n",
       "      <td>17</td>\n",
       "      <td>neu</td>\n",
       "      <td>refresh</td>\n",
       "      <td>2</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>habit</td>\n",
       "      <td>775</td>\n",
       "      <td>4.1100000000000003</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.9500000000000002</td>\n",
       "      <td>2.1099999999999999</td>\n",
       "      <td>4.2999999999999998</td>\n",
       "      <td>1.79</td>\n",
       "      <td>23</td>\n",
       "      <td>neg</td>\n",
       "      <td>repeat</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>habit</td>\n",
       "      <td>775</td>\n",
       "      <td>4.1100000000000003</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.9500000000000002</td>\n",
       "      <td>2.1099999999999999</td>\n",
       "      <td>4.2999999999999998</td>\n",
       "      <td>1.79</td>\n",
       "      <td>23</td>\n",
       "      <td>neg</td>\n",
       "      <td>repeat</td>\n",
       "      <td>2</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dump</td>\n",
       "      <td>733</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.8700000000000001</td>\n",
       "      <td>4.1200000000000001</td>\n",
       "      <td>2.3599999999999999</td>\n",
       "      <td>3.8300000000000001</td>\n",
       "      <td>1.8700000000000001</td>\n",
       "      <td>4</td>\n",
       "      <td>neg</td>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lust</td>\n",
       "      <td>519</td>\n",
       "      <td>7.1200000000000001</td>\n",
       "      <td>1.6200000000000001</td>\n",
       "      <td>6.8799999999999999</td>\n",
       "      <td>1.8500000000000001</td>\n",
       "      <td>5.4900000000000002</td>\n",
       "      <td>2.27</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>repeat</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lust</td>\n",
       "      <td>519</td>\n",
       "      <td>7.1200000000000001</td>\n",
       "      <td>1.6200000000000001</td>\n",
       "      <td>6.8799999999999999</td>\n",
       "      <td>1.8500000000000001</td>\n",
       "      <td>5.4900000000000002</td>\n",
       "      <td>2.27</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>repeat</td>\n",
       "      <td>2</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alimony</td>\n",
       "      <td>634</td>\n",
       "      <td>3.9500000000000002</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.2999999999999998</td>\n",
       "      <td>2.29</td>\n",
       "      <td>4.6299999999999999</td>\n",
       "      <td>2.2999999999999998</td>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>refresh</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>+</td>\n",
       "      <td>634</td>\n",
       "      <td>3.9500000000000002</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.2999999999999998</td>\n",
       "      <td>2.29</td>\n",
       "      <td>4.6299999999999999</td>\n",
       "      <td>2.2999999999999998</td>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>refresh</td>\n",
       "      <td>2</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dignified</td>\n",
       "      <td>118</td>\n",
       "      <td>7.0999999999999996</td>\n",
       "      <td>1.26</td>\n",
       "      <td>4.1200000000000001</td>\n",
       "      <td>2.29</td>\n",
       "      <td>6.1200000000000001</td>\n",
       "      <td>2.3999999999999999</td>\n",
       "      <td>7</td>\n",
       "      <td>pos</td>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>city</td>\n",
       "      <td>73</td>\n",
       "      <td>6.0300000000000002</td>\n",
       "      <td>1.3700000000000001</td>\n",
       "      <td>5.2400000000000002</td>\n",
       "      <td>2.5299999999999998</td>\n",
       "      <td>5.7400000000000002</td>\n",
       "      <td>2.0800000000000001</td>\n",
       "      <td>393</td>\n",
       "      <td>neu</td>\n",
       "      <td>repeat</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>city</td>\n",
       "      <td>73</td>\n",
       "      <td>6.0300000000000002</td>\n",
       "      <td>1.3700000000000001</td>\n",
       "      <td>5.2400000000000002</td>\n",
       "      <td>2.5299999999999998</td>\n",
       "      <td>5.7400000000000002</td>\n",
       "      <td>2.0800000000000001</td>\n",
       "      <td>393</td>\n",
       "      <td>neu</td>\n",
       "      <td>repeat</td>\n",
       "      <td>2</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>powerful</td>\n",
       "      <td>324</td>\n",
       "      <td>6.8399999999999999</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.8300000000000001</td>\n",
       "      <td>2.6899999999999999</td>\n",
       "      <td>7.1900000000000004</td>\n",
       "      <td>2.52</td>\n",
       "      <td>63</td>\n",
       "      <td>pos</td>\n",
       "      <td>refresh</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>+</td>\n",
       "      <td>324</td>\n",
       "      <td>6.8399999999999999</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.8300000000000001</td>\n",
       "      <td>2.6899999999999999</td>\n",
       "      <td>7.1900000000000004</td>\n",
       "      <td>2.52</td>\n",
       "      <td>63</td>\n",
       "      <td>pos</td>\n",
       "      <td>refresh</td>\n",
       "      <td>2</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description word_no        valence_mean          valence_sd  \\\n",
       "0       avenue     646                 5.5  1.3700000000000001   \n",
       "1       basket     547  5.4500000000000002  1.1499999999999999   \n",
       "2            +     547  5.4500000000000002  1.1499999999999999   \n",
       "3        habit     775  4.1100000000000003                1.77   \n",
       "4        habit     775  4.1100000000000003                1.77   \n",
       "5         dump     733                3.21  1.8700000000000001   \n",
       "6         lust     519  7.1200000000000001  1.6200000000000001   \n",
       "7         lust     519  7.1200000000000001  1.6200000000000001   \n",
       "8      alimony     634  3.9500000000000002                 2.0   \n",
       "9            +     634  3.9500000000000002                 2.0   \n",
       "10   dignified     118  7.0999999999999996                1.26   \n",
       "11        city      73  6.0300000000000002  1.3700000000000001   \n",
       "12        city      73  6.0300000000000002  1.3700000000000001   \n",
       "13    powerful     324  6.8399999999999999                 1.8   \n",
       "14           +     324  6.8399999999999999                 1.8   \n",
       "\n",
       "          arousal_mean          arousal_sd      dominance_mean  \\\n",
       "0   4.1200000000000001  2.0099999999999998  5.4000000000000004   \n",
       "1   3.6299999999999999                2.02  5.7599999999999998   \n",
       "2   3.6299999999999999                2.02  5.7599999999999998   \n",
       "3   3.9500000000000002  2.1099999999999999  4.2999999999999998   \n",
       "4   3.9500000000000002  2.1099999999999999  4.2999999999999998   \n",
       "5   4.1200000000000001  2.3599999999999999  3.8300000000000001   \n",
       "6   6.8799999999999999  1.8500000000000001  5.4900000000000002   \n",
       "7   6.8799999999999999  1.8500000000000001  5.4900000000000002   \n",
       "8   4.2999999999999998                2.29  4.6299999999999999   \n",
       "9   4.2999999999999998                2.29  4.6299999999999999   \n",
       "10  4.1200000000000001                2.29  6.1200000000000001   \n",
       "11  5.2400000000000002  2.5299999999999998  5.7400000000000002   \n",
       "12  5.2400000000000002  2.5299999999999998  5.7400000000000002   \n",
       "13  5.8300000000000001  2.6899999999999999  7.1900000000000004   \n",
       "14  5.8300000000000001  2.6899999999999999  7.1900000000000004   \n",
       "\n",
       "          dominance_sd word_frequency valence     cond  pres_num    type  \n",
       "0                 1.53             46     neu     once         1  target  \n",
       "1                 1.45             17     neu  refresh         1  target  \n",
       "2                 1.45             17     neu  refresh         2  target  \n",
       "3                 1.79             23     neg   repeat         1  target  \n",
       "4                 1.79             23     neg   repeat         2  target  \n",
       "5   1.8700000000000001              4     neg     once         1  target  \n",
       "6                 2.27              5     pos   repeat         1  target  \n",
       "7                 2.27              5     pos   repeat         2  target  \n",
       "8   2.2999999999999998              2     neg  refresh         1  target  \n",
       "9   2.2999999999999998              2     neg  refresh         2  target  \n",
       "10  2.3999999999999999              7     pos     once         1  target  \n",
       "11  2.0800000000000001            393     neu   repeat         1  target  \n",
       "12  2.0800000000000001            393     neu   repeat         2  target  \n",
       "13                2.52             63     pos  refresh         1  target  \n",
       "14                2.52             63     pos  refresh         2  target  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sl = pd.DataFrame(blocks[0]['study'])\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9edcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cond     valence\n",
       "once     neg        1\n",
       "         neu        1\n",
       "         pos        1\n",
       "refresh  neg        2\n",
       "         neu        2\n",
       "         pos        2\n",
       "repeat   neg        2\n",
       "         neu        2\n",
       "         pos        2\n",
       "Name: pres_num, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.groupby(['cond', 'valence'])['pres_num'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba99eb",
   "metadata": {},
   "source": [
    "## SMILE Experiment Details\n",
    "\n",
    "*Provide enough detail so that someone could implement the experiment presentation and response collection, including all timing information and how the blocks were structured and presented.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6c1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The condition where there is no wait time between the study and test condition \n",
    "\n",
    "from smile.common import *\n",
    "from smile.scale import scale as s\n",
    "\n",
    "# config section\n",
    "font_size = 75\n",
    "resp_keys = ['O', 'N']\n",
    "resp_map = {'target': 'O', 'lure': 'N'}\n",
    "ISI_dur = .5\n",
    "ISI_jitter = .5\n",
    "hr_jitter = 3600\n",
    "study_dur = .5\n",
    "inst_font_size = 50\n",
    "\n",
    "\n",
    "\n",
    "inst_text = '[u][size=70]Memory Recognition Test INSTRUCTIONS[/size][/u] \\nThis is a recognition memory experiment.  \\nThis means you will be presented with a list of words one at a time from a study list. \\nAfter a short delay, you will be tested for if you recall seeing that word from the previous words shown in the study list. \\nThis means when you are in the test phase of the experiment, you will be given the words from your study list, as well as new words that were not previously seen. \\nIn the test phase of each block, participants will see the study items again, along with an equal number of new items,and for each item you must specify whether the item is an old target item (i.e., one that was on the study list) or a new lure item. \\n Press ENTER key to continue.'\n",
    "# create the experiment\n",
    "exp = Experiment(name='PARTICIPANT', show_splash=False, \n",
    "                 fullscreen=False,\n",
    "                 resolution=(1024, 768), scale_box=(1024, 768))\n",
    "\n",
    "@Subroutine\n",
    "def Instruct(self):\n",
    "    # show the instructions\n",
    "    Label(text=inst_text, font_size=inst_font_size,\n",
    "          text_size=(exp.screen.width*0.75, None), \n",
    "          center=exp.screen.center, halign='center',\n",
    "          markup=True, )\n",
    "    with UntilDone():\n",
    "        KeyPress(keys=['ENTER'])\n",
    "\n",
    "    Label(text='In this test you will only need to use two keys: Press the key \"O\" to indicate the word was on the study list or an \"Old\" word. \\n Press the key \"N\" to indicate the word was not on the study list or a \"New\" word. \\nPress ENTER key to continue.',\n",
    "          font_size=font_size,\n",
    "          text_size=(exp.screen.width*0.75, None), \n",
    "          center=exp.screen.center, halign='center',\n",
    "          markup=True, )\n",
    "    with UntilDone():\n",
    "        KeyPress(keys=['ENTER'])\n",
    "\n",
    "# STUDY TRIAL WORDS APPEAR \n",
    "@Subroutine\n",
    "def StudyList(self, block_num, trial_num, study_trial):\n",
    "    # present the stimulus\n",
    "    stim = Label(text=study_trial['description'],  # Use 'description' from study_trial\n",
    "                 font_size=font_size,\n",
    "                duration = study_dur)\n",
    "\n",
    "    # wait the ISI with jitter\n",
    "    Wait(ISI_dur, jitter=ISI_jitter)\n",
    "    \n",
    "    #????????????????????????\n",
    "    # Create a study item log: Tells the researcher when each word appeared on the screen \n",
    "       \n",
    "    # log the result of the trial\n",
    "    Log(name='participant', \n",
    "        log_dict1=study_trial,\n",
    "        block_num=block_num,\n",
    "        trial_num=trial_num,\n",
    "        stim_on=ISI_dur\n",
    "       )\n",
    "    \n",
    "    \n",
    "# TEST TRIAL WORDS APPEAR \n",
    "@Subroutine\n",
    "def TestList(self, block_num, trial_num, test_trial):\n",
    "    # present the stimulus\n",
    "    stim = Label(text=test_trial['description'],  # Use 'description' from test_trial\n",
    "                 font_size=font_size)\n",
    "    \n",
    "    with UntilDone():\n",
    "        Wait(until=stim.appear_time)\n",
    "        kp = KeyPress(keys=resp_keys, \n",
    "                      base_time=stim.appear_time['time'],\n",
    "                      correct_resp=Ref.object(resp_map)[test_trial['type']])  # Use 'type' from test_trial\n",
    "    \n",
    "    # log the result of the trial\n",
    "    Log(name='participant', \n",
    "        log_dict=test_trial,\n",
    "        block_num=block_num,\n",
    "        trial_num=trial_num,\n",
    "        stim_on=stim.appear_time,\n",
    "        resp=kp.pressed,\n",
    "        resp_time=kp.press_time,\n",
    "        rt=kp.rt,\n",
    "        correct=kp.correct,\n",
    "       )\n",
    "\n",
    "\n",
    "# Start of Simulation: \n",
    "Label(text='Welcome to the Memory Recognition Test! \\nPress ENTER key to Continue',\n",
    "      font_size=font_size)\n",
    "with UntilDone():\n",
    "    KeyPress(keys=['ENTER'])\n",
    "\n",
    "# Get the subj id information\n",
    "from smile.startup import InputSubject\n",
    "\n",
    "InputSubject('Participant')\n",
    "\n",
    "# show the instructions\n",
    "Instruct()\n",
    "Wait(0.5)\n",
    "    \n",
    "# loop over the blocks\n",
    "with Loop(blocks) as block:\n",
    "    # make sure they are ready to continue\n",
    "    Label(text='Press the ENTER key to\\nstart the next block.', \n",
    "          font_size=font_size, halign='center')\n",
    "    with UntilDone():\n",
    "        KeyPress(keys=['ENTER'])\n",
    "\n",
    "    # add in some delay before the start of the block\n",
    "    Wait(ISI_dur, jitter=ISI_jitter)\n",
    "    \n",
    "    # loop over the study trial\n",
    "    with Loop(block.current['study']) as study_trial:\n",
    "        StudyList(block.i, study_trial.i, study_trial.current)\n",
    "    \n",
    "    # add in some delay before the start of the test block (two conditions)\n",
    "    Wait(ISI_dur, jitter=ISI_jitter)\n",
    "    \n",
    "    # OR \n",
    "    #Wait(ISI_dur, jitter=hr_jitter)\n",
    "\n",
    "    # loop over the test trial\n",
    "    with Loop(block.current['test']) as test_trial:\n",
    "        TestList(block.i, test_trial.i, test_trial.current)\n",
    "    \n",
    "\n",
    "# make sure they are ready to continue\n",
    "Label(text='You are all done!!!\\nPress the ENTER key to go celebrate.', \n",
    "      font_size=font_size, halign='center')\n",
    "with UntilDone():\n",
    "    KeyPress(keys=['ENTER'])\n",
    "\n",
    "# run the experiment\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f4c8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The condition where there is an hour wait time between the study list and the test list  \n",
    "\n",
    "from smile.common import *\n",
    "from smile.scale import scale as s\n",
    "\n",
    "# config section\n",
    "font_size = 75\n",
    "resp_keys = ['O', 'N']\n",
    "resp_map = {'target': 'O', 'lure': 'N'}\n",
    "ISI_dur = .5\n",
    "ISI_jitter = .5\n",
    "hr_jitter = 3600\n",
    "study_dur = .5\n",
    "inst_font_size = 50\n",
    "\n",
    "\n",
    "\n",
    "inst_text = '[u][size=70]Memory Recognition Test INSTRUCTIONS[/size][/u] \\nThis is a recognition memory experiment.  \\nThis means you will be presented with a list of words one at a time from a study list. \\nAfter a short delay, you will be tested for if you recall seeing that word from the previous words shown in the study list. \\nThis means when you are in the test phase of the experiment, you will be given the words from your study list, as well as new words that were not previously seen. \\nIn the test phase of each block, participants will see the study items again, along with an equal number of new items,and for each item you must specify whether the item is an old target item (i.e., one that was on the study list) or a new lure item. \\n Press ENTER key to continue.'\n",
    "# create the experiment\n",
    "exp = Experiment(name='PARTICIPANT', show_splash=False, \n",
    "                 fullscreen=False,\n",
    "                 resolution=(1024, 768), scale_box=(1024, 768))\n",
    "\n",
    "@Subroutine\n",
    "def Instruct(self):\n",
    "    # show the instructions\n",
    "    Label(text=inst_text, font_size=inst_font_size,\n",
    "          text_size=(exp.screen.width*0.75, None), \n",
    "          center=exp.screen.center, halign='center',\n",
    "          markup=True, )\n",
    "    with UntilDone():\n",
    "        KeyPress(keys=['ENTER'])\n",
    "\n",
    "    Label(text='In this test you will only need to use two keys: Press the key \"O\" to indicate the word was on the study list or an \"Old\" word. \\n Press the key \"N\" to indicate the word was not on the study list or a \"New\" word. \\nPress ENTER key to continue.',\n",
    "          font_size=font_size,\n",
    "          text_size=(exp.screen.width*0.75, None), \n",
    "          center=exp.screen.center, halign='center',\n",
    "          markup=True, )\n",
    "    with UntilDone():\n",
    "        KeyPress(keys=['ENTER'])\n",
    "\n",
    "# STUDY TRIAL WORDS APPEAR \n",
    "@Subroutine\n",
    "def StudyList(self, block_num, trial_num, study_trial):\n",
    "    # present the stimulus\n",
    "    stim = Label(text=study_trial['description'],  # Use 'description' from study_trial\n",
    "                 font_size=font_size,\n",
    "                duration = study_dur)\n",
    "\n",
    "    # wait the ISI with jitter\n",
    "    Wait(ISI_dur, jitter=ISI_jitter)\n",
    "    \n",
    "    #????????????????????????\n",
    "    # Create a study item log: Tells the researcher when each word appeared on the screen \n",
    "       \n",
    "    # log the result of the trial\n",
    "    Log(name='participant', \n",
    "        log_dict1=study_trial,\n",
    "        block_num=block_num,\n",
    "        trial_num=trial_num,\n",
    "        stim_on=ISI_dur\n",
    "       )\n",
    "    \n",
    "    \n",
    "# TEST TRIAL WORDS APPEAR \n",
    "@Subroutine\n",
    "def TestList(self, block_num, trial_num, test_trial):\n",
    "    # present the stimulus\n",
    "    stim = Label(text=test_trial['description'],  # Use 'description' from test_trial\n",
    "                 font_size=font_size)\n",
    "    \n",
    "    with UntilDone():\n",
    "        Wait(until=stim.appear_time)\n",
    "        kp = KeyPress(keys=resp_keys, \n",
    "                      base_time=stim.appear_time['time'],\n",
    "                      correct_resp=Ref.object(resp_map)[test_trial['type']])  # Use 'type' from test_trial\n",
    "    \n",
    "    # log the result of the trial\n",
    "    Log(name='participant', \n",
    "        log_dict=test_trial,\n",
    "        block_num=block_num,\n",
    "        trial_num=trial_num,\n",
    "        stim_on=stim.appear_time,\n",
    "        resp=kp.pressed,\n",
    "        resp_time=kp.press_time,\n",
    "        rt=kp.rt,\n",
    "        correct=kp.correct,\n",
    "       )\n",
    "\n",
    "\n",
    "# Start of Simulation: \n",
    "Label(text='Welcome to the Memory Recognition Test! \\nPress ENTER key to Continue',\n",
    "      font_size=font_size)\n",
    "with UntilDone():\n",
    "    KeyPress(keys=['ENTER'])\n",
    "\n",
    "# Get the subj id information\n",
    "from smile.startup import InputSubject\n",
    "\n",
    "InputSubject('Participant')\n",
    "\n",
    "# show the instructions\n",
    "Instruct()\n",
    "Wait(0.5)\n",
    "    \n",
    "# loop over the blocks\n",
    "with Loop(blocks) as block:\n",
    "    # make sure they are ready to continue\n",
    "    Label(text='Press the ENTER key to\\nstart the next block.', \n",
    "          font_size=font_size, halign='center')\n",
    "    with UntilDone():\n",
    "        KeyPress(keys=['ENTER'])\n",
    "\n",
    "    # add in some delay before the start of the block\n",
    "    Wait(ISI_dur, jitter=ISI_jitter)\n",
    "    \n",
    "    # loop over the study trial\n",
    "    with Loop(block.current['study']) as study_trial:\n",
    "        StudyList(block.i, study_trial.i, study_trial.current)\n",
    "    \n",
    "    # add in some delay before the start of the test block (two conditions)\n",
    "    # Wait(ISI_dur, jitter=ISI_jitter)\n",
    "    \n",
    "    # OR \n",
    "    Wait(ISI_dur, jitter=hr_jitter)\n",
    "\n",
    "    # loop over the test trial\n",
    "    with Loop(block.current['test']) as test_trial:\n",
    "        TestList(block.i, test_trial.i, test_trial.current)\n",
    "    \n",
    "\n",
    "# make sure they are ready to continue\n",
    "Label(text='You are all done!!!\\nPress the ENTER key to go celebrate.', \n",
    "      font_size=font_size, halign='center')\n",
    "with UntilDone():\n",
    "    KeyPress(keys=['ENTER'])\n",
    "\n",
    "# run the experiment\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062cd99",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "*In this section, state a specific question, then define your dependent and independent variables that will help you answer that question. As stated above, your question must give rise to an analysis that is not identical to one we performed in class (i.e., you must do more than copy and paste code with zero changes. That said, the analysis can match those from the class quite closely.*\n",
    "\n",
    "*Note: You need to repeat this process for a total of two hypotheses/questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9173940",
   "metadata": {},
   "source": [
    "## Data processing and visualization\n",
    "\n",
    "*With the lessons as a guide, process your data to create the necessary data frame to plot the visualization associated with the question stated above. Then plot those data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2c57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for data processing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cf8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to load slogs\n",
    "def load_all_subj_logs(data_dir, log_file):\n",
    "    # load in a list of all the subj\n",
    "    subjs = [os.path.splitext(os.path.split(filepath)[1])[0].split('_')[-1]\n",
    "             for filepath in glob(os.path.join(data_dir, log_file + '*.slog'))]\n",
    "    subjs.sort()\n",
    "\n",
    "    # loop over subj and their data\n",
    "    all_dat = []\n",
    "    for subj in subjs:\n",
    "        # set the file\n",
    "        log_path = os.path.join(data_dir, log_file+'_'+subj+'.slog')\n",
    "        #print(log_path)\n",
    "\n",
    "        # load the data\n",
    "        try:\n",
    "            all_dat.extend(log2dl(log_path, subj=subj))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(all_dat)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881503ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b5de535",
   "metadata": {},
   "source": [
    "*Put text here describing what is in your plot like a detailed figure caption.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b393b",
   "metadata": {},
   "source": [
    "## Statistical test and interpretation\n",
    "\n",
    "*Perform a statistical test to support your conclusions with regard to your question outlined above. This can be with either statsmodels or with bambi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "badc89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for statistical test (can be either with statsmodels or bambi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1512d",
   "metadata": {},
   "source": [
    "*Put text here describing the results of your statistical test*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-smile]",
   "language": "python",
   "name": "conda-env-anaconda3-smile-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
